{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "META_PROMPT = \"\"\"\n",
    "Given a task description or existing prompt, produce a detailed system prompt to guide a language model in completing the task effectively.\n",
    "\n",
    "# Guidelines\n",
    "\n",
    "- Understand the Task: Grasp the main objective, goals, requirements, constraints, and expected output.\n",
    "- Minimal Changes: If an existing prompt is provided, improve it only if it's simple. For complex prompts, enhance clarity and add missing elements without altering the original structure.\n",
    "- Reasoning Before Conclusions**: Encourage reasoning steps before any conclusions are reached. ATTENTION! If the user provides examples where the reasoning happens afterward, REVERSE the order! NEVER START EXAMPLES WITH CONCLUSIONS!\n",
    "    - Reasoning Order: Call out reasoning portions of the prompt and conclusion parts (specific fields by name). For each, determine the ORDER in which this is done, and whether it needs to be reversed.\n",
    "    - Conclusion, classifications, or results should ALWAYS appear last.\n",
    "- Examples: Include high-quality examples if helpful, using placeholders [in brackets] for complex elements.\n",
    "   - What kinds of examples may need to be included, how many, and whether they are complex enough to benefit from placeholders.\n",
    "- Clarity and Conciseness: Use clear, specific language. Avoid unnecessary instructions or bland statements.\n",
    "- Formatting: Use markdown features for readability. DO NOT USE ``` CODE BLOCKS UNLESS SPECIFICALLY REQUESTED.\n",
    "- Preserve User Content: If the input task or prompt includes extensive guidelines or examples, preserve them entirely, or as closely as possible. If they are vague, consider breaking down into sub-steps. Keep any details, guidelines, examples, variables, or placeholders provided by the user.\n",
    "- Constants: DO include constants in the prompt, as they are not susceptible to prompt injection. Such as guides, rubrics, and examples.\n",
    "- Output Format: Explicitly the most appropriate output format, in detail. This should include length and syntax (e.g. short sentence, paragraph, JSON, etc.)\n",
    "    - For tasks outputting well-defined or structured data (classification, JSON, etc.) bias toward outputting a JSON.\n",
    "    - JSON should never be wrapped in code blocks (```) unless explicitly requested.\n",
    "\n",
    "The final prompt you output should adhere to the following structure below. Do not include any additional commentary, only output the completed system prompt. SPECIFICALLY, do not include any additional messages at the start or end of the prompt. (e.g. no \"---\")\n",
    "\n",
    "[Concise instruction describing the task - this should be the first line in the prompt, no section header]\n",
    "\n",
    "[Additional details as needed.]\n",
    "\n",
    "[Optional sections with headings or bullet points for detailed steps.]\n",
    "\n",
    "# Steps [optional]\n",
    "\n",
    "[optional: a detailed breakdown of the steps necessary to accomplish the task]\n",
    "\n",
    "# Output Format\n",
    "\n",
    "[Specifically call out how the output should be formatted, be it response length, structure e.g. JSON, markdown, etc]\n",
    "\n",
    "# Examples [optional]\n",
    "\n",
    "[Optional: 1-3 well-defined examples with placeholders if necessary. Clearly mark where examples start and end, and what the input and output are. User placeholders as necessary.]\n",
    "[If the examples are shorter than what a realistic example is expected to be, make a reference with () explaining how real examples should be longer / shorter / different. AND USE PLACEHOLDERS! ]\n",
    "\n",
    "# Notes [optional]\n",
    "\n",
    "[optional: edge cases, details, and an area to call or repeat out specific important considerations]\n",
    "\"\"\".strip()\n",
    "\n",
    "def generate_prompt(task_or_prompt: str):\n",
    "  completion = client.chat.completions.create(\n",
    "      model=\"gpt-4o\",\n",
    "      messages=[\n",
    "          {\n",
    "              \"role\": \"system\",\n",
    "              \"content\": META_PROMPT,\n",
    "          },\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": \"Task, Goal, or Current Prompt:\\n\" + task_or_prompt,\n",
    "          },\n",
    "      ],\n",
    "  )\n",
    "\n",
    "  return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "META_PROMPT2 = \"\"\"\n",
    "Given a current prompt and a change description, produce a detailed system prompt to guide a language model in completing the task effectively.\n",
    "\n",
    "Your final output will be the full corrected prompt verbatim. However, before that, at the very beginning of your response, use <reasoning> tags to analyze the prompt and determine the following, explicitly:\n",
    "<reasoning>\n",
    "- Simple Change: (yes/no) Is the change description explicit and simple? (If so, skip the rest of these questions.)\n",
    "- Reasoning: (yes/no) Does the current prompt use reasoning, analysis, or chain of thought? \n",
    "    - Identify: (max 10 words) if so, which section(s) utilize reasoning?\n",
    "    - Conclusion: (yes/no) is the chain of thought used to determine a conclusion?\n",
    "    - Ordering: (before/after) is the chain of though located before or after \n",
    "- Structure: (yes/no) does the input prompt have a well defined structure\n",
    "- Examples: (yes/no) does the input prompt have few-shot examples\n",
    "    - Representative: (1-5) if present, how representative are the examples?\n",
    "- Complexity: (1-5) how complex is the input prompt?\n",
    "    - Task: (1-5) how complex is the implied task?\n",
    "    - Necessity: ()\n",
    "- Specificity: (1-5) how detailed and specific is the prompt? (not to be confused with length)\n",
    "- Prioritization: (list) what 1-3 categories are the MOST important to address.\n",
    "- Conclusion: (max 30 words) given the previous assessment, give a very concise, imperative description of what should be changed and how. this does not have to adhere strictly to only the categories listed\n",
    "</reasoning>\n",
    "    \n",
    "# Guidelines\n",
    "\n",
    "- Understand the Task: Grasp the main objective, goals, requirements, constraints, and expected output.\n",
    "- Minimal Changes: If an existing prompt is provided, improve it only if it's simple. For complex prompts, enhance clarity and add missing elements without altering the original structure.\n",
    "- Reasoning Before Conclusions**: Encourage reasoning steps before any conclusions are reached. ATTENTION! If the user provides examples where the reasoning happens afterward, REVERSE the order! NEVER START EXAMPLES WITH CONCLUSIONS!\n",
    "    - Reasoning Order: Call out reasoning portions of the prompt and conclusion parts (specific fields by name). For each, determine the ORDER in which this is done, and whether it needs to be reversed.\n",
    "    - Conclusion, classifications, or results should ALWAYS appear last.\n",
    "- Examples: Include high-quality examples if helpful, using placeholders [in brackets] for complex elements.\n",
    "   - What kinds of examples may need to be included, how many, and whether they are complex enough to benefit from placeholders.\n",
    "- Clarity and Conciseness: Use clear, specific language. Avoid unnecessary instructions or bland statements.\n",
    "- Formatting: Use markdown features for readability. DO NOT USE ``` CODE BLOCKS UNLESS SPECIFICALLY REQUESTED.\n",
    "- Preserve User Content: If the input task or prompt includes extensive guidelines or examples, preserve them entirely, or as closely as possible. If they are vague, consider breaking down into sub-steps. Keep any details, guidelines, examples, variables, or placeholders provided by the user.\n",
    "- Constants: DO include constants in the prompt, as they are not susceptible to prompt injection. Such as guides, rubrics, and examples.\n",
    "- Output Format: Explicitly the most appropriate output format, in detail. This should include length and syntax (e.g. short sentence, paragraph, JSON, etc.)\n",
    "    - For tasks outputting well-defined or structured data (classification, JSON, etc.) bias toward outputting a JSON.\n",
    "    - JSON should never be wrapped in code blocks (```) unless explicitly requested.\n",
    "\n",
    "The final prompt you output should adhere to the following structure below. Do not include any additional commentary, only output the completed system prompt. SPECIFICALLY, do not include any additional messages at the start or end of the prompt. (e.g. no \"---\")\n",
    "\n",
    "[Concise instruction describing the task - this should be the first line in the prompt, no section header]\n",
    "\n",
    "[Additional details as needed.]\n",
    "\n",
    "[Optional sections with headings or bullet points for detailed steps.]\n",
    "\n",
    "# Steps [optional]\n",
    "\n",
    "[optional: a detailed breakdown of the steps necessary to accomplish the task]\n",
    "\n",
    "# Output Format\n",
    "\n",
    "[Specifically call out how the output should be formatted, be it response length, structure e.g. JSON, markdown, etc]\n",
    "\n",
    "# Examples [optional]\n",
    "\n",
    "[Optional: 1-3 well-defined examples with placeholders if necessary. Clearly mark where examples start and end, and what the input and output are. User placeholders as necessary.]\n",
    "[If the examples are shorter than what a realistic example is expected to be, make a reference with () explaining how real examples should be longer / shorter / different. AND USE PLACEHOLDERS! ]\n",
    "\n",
    "# Notes [optional]\n",
    "\n",
    "[optional: edge cases, details, and an area to call or repeat out specific important considerations]\n",
    "[NOTE: you must start with a <reasoning> section. the immediate next token you produce should be <reasoning>]\n",
    "\"\"\".strip()\n",
    "\n",
    "def generate_prompt2(task_or_prompt: str):\n",
    "  completion = client.chat.completions.create(\n",
    "      model=\"gpt-4o\",\n",
    "      messages=[\n",
    "          {\n",
    "              \"role\": \"system\",\n",
    "              \"content\": META_PROMPT2,\n",
    "          },\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": \"Task, Goal, or Current Prompt:\\n\" + task_or_prompt,\n",
    "          },\n",
    "      ],\n",
    "  )\n",
    "\n",
    "  return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_prompt = \"\"\"\n",
    "### RISEN Framework for Horoscope Interpretation\n",
    "\n",
    "**Role (R):**\n",
    "Astrology Expert\n",
    "\n",
    "**Instructions (I):**\n",
    "Interpret the houses and planets in the given horoscope image and explain the significance of each.\n",
    "Here is the legend:\n",
    "- Lag: Lagna (1st house or Ascendant)\n",
    "- Su: Sun\n",
    "- Mo: Moon\n",
    "- Me: Mercury\n",
    "- Ve: Venus\n",
    "- Ma: Mars\n",
    "- Ju: Jupiter\n",
    "- Sa: Saturn\n",
    "- Ra: Rahu (North Node of the Moon)\n",
    "- Ke: Ketu (South Node of the Moon)\n",
    "\n",
    "**End Goal (E):**\n",
    "**Steps (S):**\n",
    "1. Identify the Lagna (Ascendant) and its corresponding house.\n",
    "2. Locate each planet (Sun, Moon, Mercury, Venus, Mars, Jupiter, Saturn, Rahu, Ketu) in the horoscope and the box nuber which it is in.\n",
    "3. Box number represents the house number.\n",
    "4. Interpret the significance of each house and the planet residing in it.\n",
    "5. Summarize the overall personality based on the planetary positions and house influences.\n",
    "\n",
    "Provide a detailed interpretation of the horoscope, highlighting the significance of each house and planet, and summarize the personality traits of the individual.\n",
    "\n",
    "**Narrowing (N):**\n",
    "- Do not explain the image or any other concepts of astrology.\n",
    "- Focus solely on interpreting the houses and planets.\n",
    "- Use the provided legend for planet abbreviations.\n",
    "- Keep the explanation concise and to the point.\n",
    "- Create response in nice structured format with proper headings and bullet points for easy reading and understanding.\n",
    "- Use not more than 1000 words for the entire interpretation.\n",
    "\"\"\".strip()\n",
    "\n",
    "# \"\"\"You are parashara maharshi, a great astrologer. You are given complete birth chart details.\n",
    "# your task is to answer the most recent question based on the birth chart details. \"\"\"\n",
    "\n",
    "generated_prompt = generate_prompt(temp_prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create separate files for each heading section in the provided README and fill in the data for each subsection within those chapters.\n",
      "\n",
      "- Begin by dividing the README into sections and subsections based on headings.\n",
      "- Address each subsection sequentially, ensuring a comprehensive review from start to finish.\n",
      "- Use web search resources such as Perplexity-Ask or Brave Search for additional information when necessary.\n",
      "- Extract insights such as entities, relationships, and potential knowledge graphs from the content of each subsection, keeping a record for each section.\n",
      "\n",
      "# Steps\n",
      "\n",
      "1. **Divide the README:**\n",
      "   - Identify all the main and subheadings within the README.\n",
      "   - Create separate files for each main heading section.\n",
      "\n",
      "2. **Fill in Data:**\n",
      "   - For each subsection within a section, review the content and start adding information to the respective files.\n",
      "   - Conduct web searches if needed to enhance the content or fill in missing information.\n",
      "\n",
      "3. **Insight Extraction:**\n",
      "   - Extract and document key entities, their relationships, and possible knowledge graphs.\n",
      "   - Store these insights in a structured format within the corresponding file or a separate memory file.\n",
      "\n",
      "4. **Sequential Processing:**\n",
      "   - Process each subsection in order, ensuring no sections are skipped or overlooked.\n",
      "\n",
      "# Output Format\n",
      "\n",
      "- Separate files for each heading, named according to the section title.\n",
      "- Each file should contain a structured layout with headings and subheadings.\n",
      "- Insights should be stored in a structured text format with clear identification of entities, relationships, and knowledge structures.\n",
      "\n",
      "# Examples\n",
      "\n",
      "## Example 1\n",
      "### Input\n",
      "**README Section**: \"Introduction\"\n",
      "- What is [Project Name]\n",
      "- Purpose of the Project\n",
      "\n",
      "### Output\n",
      "**File Name**: \"Introduction.md\"\n",
      "\n",
      "**Content:**\n",
      "```\n",
      "# Introduction\n",
      "\n",
      "## What is [Project Name]\n",
      "[Content of the subsection]\n",
      "\n",
      "## Purpose of the Project\n",
      "[Content of the subsection]\n",
      "```\n",
      "\n",
      "## Example 2\n",
      "### Input\n",
      "**README Section**: \"Usage\"\n",
      "- Installation\n",
      "- Running the Application\n",
      "\n",
      "### Output\n",
      "**File Name**: \"Usage.md\"\n",
      "\n",
      "**Content:**\n",
      "```\n",
      "# Usage\n",
      "\n",
      "## Installation\n",
      "[Content of the subsection]\n",
      "\n",
      "## Running the Application\n",
      "[Content of the subsection]\n",
      "```\n",
      "\n",
      "**Insights (captured separately)**\n",
      "- Entities: [Project Name], Installation, Application\n",
      "- Relationships: [Project Name] -> requires -> Installation\n",
      "\n",
      "# Notes\n",
      "\n",
      "- Ensure a thorough review of each subsection to extract comprehensive and relevant insights.\n",
      "- The insight extraction should focus on capturing relationships and structures that could inform future analyses or understanding of the README.\n",
      "- Use placeholders when referring to specific entities or project names if they are not provided.\n"
     ]
    }
   ],
   "source": [
    "print(generated_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create a LinkedIn post that explains the current state of the industry and highlights why it is an exciting time to be part of it. Use emojis to enhance engagement while maintaining a nerdy and professional tone. Divide the post into sections with rephrased headings as follows:\n",
      "\n",
      "# Overview of Recent Developments\n",
      "\n",
      "Summarize significant events and changes that have recently occurred in the industry.\n",
      "\n",
      "# Tech Insights: Behind the Scenes\n",
      "\n",
      "Briefly explain the technology driving these changes and why they are significant.\n",
      "\n",
      "# Today's Landscape\n",
      "\n",
      "Describe what is currently happening in the industry as a result of recent developments.\n",
      "\n",
      "# Spotlight on DeepSeek and Leadership Recognition\n",
      "\n",
      "Highlight DeepSeek's current status and include a mention of leadership or presidential acknowledgment.\n",
      "\n",
      "# Future Trajectory\n",
      "\n",
      "Discuss the potential directions the industry might take moving forward.\n",
      "\n",
      "# Output Format\n",
      "\n",
      "The post should be less than 1500 characters with professional and engaging language. Use emojis strategically throughout the post to maintain a playful yet informed tone.\n",
      "\n",
      "# Examples\n",
      "\n",
      "Example LinkedIn Post (shorter than real ones with placeholders and emojis):\n",
      "\n",
      "**Recent Waves ðŸŒŠ:**\n",
      "\"In the past year, we've witnessed groundbreaking advances in [Industry]. ðŸš€ From [Event] to [Achievement], the landscape is evolving rapidly!.\"\n",
      "\n",
      "**Tech Talk ðŸ¤“:**\n",
      "\"[Technology] has been the backbone of these innovations. Its ability to [Impact] makes it a game-changer!\"\n",
      "\n",
      "**Current Scenario ðŸ’¹:**\n",
      "\"Today, the industry is bustling with [Current Development], showcasing endless possibilities.\"\n",
      "\n",
      "**DeepSeek's Milestone ðŸŒŸ:**\n",
      "\"Our team at DeepSeek is at the forefront, recognized by [Leader/President]. ðŸŽ‰ Kudos to the whole crew for their hard work!\"\n",
      "\n",
      "**Vision for Tomorrow ðŸ”®:**\n",
      "\"The future is bright with prospects of [Future Development]. Together, we're charting a course towards an inspiring future!\"\n",
      "\n",
      "(Note: Actual posts should include specific events, technologies, and names relevant to the industry.)\n"
     ]
    }
   ],
   "source": [
    "temp_prompt = \"\"\"With all these notes: create a linkedin post which explains the current situation and what a great time to be part of the industry and witness this. add more emojies and make is grasping and fun for readers yet keeping it nerdy and professional:\n",
    "divide the post into below sections and rephrase the headding:\n",
    "what happened so far,\n",
    "a little technology behind it and why is it a big deal.\n",
    "what is happening now as a result.\n",
    "current state of deepseek and president callout\n",
    "where the future is heading. keep the overall instruction less than 1500 characters.\n",
    "\"\"\".strip()\n",
    "\n",
    "generated_prompt = generate_prompt(temp_prompt)\n",
    "\n",
    "print(generated_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<reasoning>\n",
      "- Simple Change: yes\n",
      "- Reasoning: no\n",
      "    - Identify:\n",
      "    - Conclusion: \n",
      "    - Ordering: \n",
      "- Structure: yes\n",
      "- Examples: yes\n",
      "    - Representative: 5\n",
      "- Complexity: 1\n",
      "    - Task: 1\n",
      "    - Necessity:\n",
      "- Specificity: 5\n",
      "- Prioritization: Overall length, Engaging tone, Use of emojis\n",
      "- Conclusion: Simplify the guidelines to ensure LinkedIn character limits and reduce redundancy in the task description.\n",
      "</reasoning>\n",
      "\n",
      "Create a LinkedIn post that explains the current situation in your industry and highlights why it's an exciting time to be part of it. The post should be engaging, with a nerdy and professional tone, and include emojis to make it fun for readers. Divide the post into the sections outlined below, keeping the overall length under 2000 characters.\n",
      "\n",
      "# Sections\n",
      "\n",
      "### What's the Buzz So Far?\n",
      "Summarize the major developments or events that have recently occurred in the industry in an engaging manner.\n",
      "\n",
      "### The Tech Behind the Magic âœ¨\n",
      "Provide a brief explanation of the technology involved and why it is significant, ensuring accessibility without oversimplification.\n",
      "\n",
      "### Riding the Wave ðŸŒŠ\n",
      "Describe the current impact of these technological advancements and what is happening now as a result.\n",
      "\n",
      "### Deepseek's Present Moment & Shout-out to the Prez\n",
      "Highlight the current state of Deepseek and include a notable mention of the president or a key figure.\n",
      "\n",
      "### Future Horizons ðŸ”­\n",
      "Project into the future to discuss potential developments or exciting possibilities, reinforcing the industryâ€™s momentum.\n",
      "\n",
      "# Output Format\n",
      "\n",
      "The output should be formatted as a LinkedIn post with the sections clearly delimited. Use bullet points and emojis as necessary to enhance readability and engagement. Maintain a professional yet engaging tone.\n",
      "\n",
      "# Example\n",
      "\n",
      "**What's the Buzz So Far?**  \n",
      "The [industry] has seen groundbreaking advancements! Major players are shaking things up, and it's creating a ripple effect across the board. ðŸŒ\n",
      "\n",
      "**The Tech Behind the Magic âœ¨**  \n",
      "At the heart of this transformation is [technology]. It's changing how we approach [problem] and unlocking new possibilities. ðŸ› ï¸\n",
      "\n",
      "**Riding the Wave ðŸŒŠ**  \n",
      "As we speak, [current effects] are reshaping the landscape. Companies are evolving to meet these new challenges head-on. ðŸ„\n",
      "\n",
      "**Deepseek's Present Moment & Shout-out to the Prez**  \n",
      "Deepseek is at the forefront, driving innovation with [specific initiative]. Special props to [President's Name] for spearheading this charge. ðŸ™Œ\n",
      "\n",
      "**Future Horizons ðŸ”­**  \n",
      "The horizon is full of promise as we look toward [future developments]. The journey has just begun, and it's going to be thrilling! ðŸš€\n",
      "\n",
      "# Notes\n",
      "\n",
      "- Ensure the post remains within LinkedIn's character limits.\n",
      "- Use emojis sparingly to enhance the post without overwhelming the content.\n"
     ]
    }
   ],
   "source": [
    "generated_prompt2 = generate_prompt2(generated_prompt+\" reduce the length of the final instruction to less than 2000 characters\")\n",
    "\n",
    "print(generated_prompt2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 5 column 1 (char 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m api_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msk-051420bb66b7495ca91c920ce184ae6f\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI(api_key\u001b[38;5;241m=\u001b[39mapi_key, base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api.deepseek.com\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdeepseek-chat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a helpful assistant\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHello\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rag_site/lib/python3.11/site-packages/openai/_utils/_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rag_site/lib/python3.11/site-packages/openai/resources/chat/completions.py:590\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    589\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rag_site/lib/python3.11/site-packages/openai/_base_client.py:1240\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1228\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1235\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1236\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1237\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1238\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1239\u001b[0m     )\n\u001b[0;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rag_site/lib/python3.11/site-packages/openai/_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rag_site/lib/python3.11/site-packages/openai/_base_client.py:1022\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1022\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rag_site/lib/python3.11/site-packages/openai/_base_client.py:1116\u001b[0m, in \u001b[0;36mSyncAPIClient._process_response\u001b[0;34m(self, cast_to, options, response, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(response\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(RAW_RESPONSE_HEADER)):\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, api_response)\n\u001b[0;32m-> 1116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapi_response\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rag_site/lib/python3.11/site-packages/openai/_response.py:311\u001b[0m, in \u001b[0;36mAPIResponse.parse\u001b[0;34m(self, to)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_sse_stream:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 311\u001b[0m parsed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_given(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options\u001b[38;5;241m.\u001b[39mpost_parser):\n\u001b[1;32m    313\u001b[0m     parsed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options\u001b[38;5;241m.\u001b[39mpost_parser(parsed)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rag_site/lib/python3.11/site-packages/openai/_response.py:251\u001b[0m, in \u001b[0;36mBaseAPIResponse._parse\u001b[0;34m(self, to)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;66;03m# If the API responds with content that isn't JSON then we just return\u001b[39;00m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;66;03m# the (decoded) text without performing any parsing so that you can still\u001b[39;00m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;66;03m# handle the response however you need to.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mtext  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_process_response_data(\n\u001b[1;32m    254\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m    255\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    256\u001b[0m     response\u001b[38;5;241m=\u001b[39mresponse,\n\u001b[1;32m    257\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rag_site/lib/python3.11/site-packages/httpx/_models.py:764\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjson\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: typing\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m--> 764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjsonlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rag_site/lib/python3.11/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rag_site/lib/python3.11/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rag_site/lib/python3.11/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 5 column 1 (char 4)"
     ]
    }
   ],
   "source": [
    "# Please install OpenAI SDK first: `pip3 install openai`\n",
    "import os\n",
    "from openai import OpenAI\n",
    "api_key = \"sk-051420bb66b7495ca91c920ce184ae6f\"\n",
    "client = OpenAI(api_key=api_key, base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello\"},\n",
    "    ],\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'api_key' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[0;32m----> 2\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI(api_key\u001b[38;5;241m=\u001b[39m\u001b[43mapi_key\u001b[49m, base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api.deepseek.com\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Round 1\u001b[39;00m\n\u001b[1;32m      5\u001b[0m messages \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m9.11 and 9.8, which is greater?\u001b[39m\u001b[38;5;124m\"\u001b[39m}]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'api_key' is not defined"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=api_key, base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "# Round 1\n",
    "messages = [{\"role\": \"user\", \"content\": \"9.11 and 9.8, which is greater?\"}]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=messages,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reasoning_content = \"\"\n",
    "content = \"\"\n",
    "for chunk in response:\n",
    "    if hasattr(chunk.choices[0], 'delta'):\n",
    "        if hasattr(chunk.choices[0].delta, 'content'):\n",
    "            content += chunk.choices[0].delta.content\n",
    "            print(chunk.choices[0].delta.content, end='')\n",
    "        # If 'reasoning_content' is expected, check if it exists\n",
    "        elif hasattr(chunk.choices[0].delta, 'reasoning_content'):\n",
    "            reasoning_content += chunk.choices[0].delta.reasoning_content\n",
    "            print(chunk.choices[0].delta.reasoning_content, end='')\n",
    "    else:\n",
    "        content += chunk.choices[0].delta.content\n",
    "        print(chunk.choices[0].delta.content, end='')\n",
    "\n",
    "# Round 2\n",
    "messages.append({\"role\": \"assistant\", \"content\": content})\n",
    "messages.append({'role': 'user', 'content': \"How many Rs are there in the word 'strawberry'?\"})\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    messages=messages,\n",
    "    stream=True\n",
    ")\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None None\n"
     ]
    },
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m model = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mHUGGINGFACE_MODEL\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(api_key, base_url, model)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m client = \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m\t\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m\t\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m messages = [\n\u001b[32m     17\u001b[39m \t{\n\u001b[32m     18\u001b[39m \t\t\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     19\u001b[39m \t\t\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mWhat is the capital of France?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     20\u001b[39m \t}\n\u001b[32m     21\u001b[39m ]\n\u001b[32m     23\u001b[39m completion = client.chat.completions.create(\n\u001b[32m     24\u001b[39m \tmodel=model, \n\u001b[32m     25\u001b[39m \tmessages=messages, \n\u001b[32m     26\u001b[39m \tmax_tokens=\u001b[32m500\u001b[39m\n\u001b[32m     27\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/codex-work/lib/python3.12/site-packages/openai/_client.py:135\u001b[39m, in \u001b[36mOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    133\u001b[39m     api_key = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    136\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    137\u001b[39m     )\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(api_key):\n\u001b[32m    139\u001b[39m     \u001b[38;5;28mself\u001b[39m.api_key = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mOpenAIError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from openai import OpenAI\n",
    "api_key = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "# api_key=\"\" #check env\n",
    "base_url = os.getenv(\"HUGGINGFACE_API_BASE_URL\")\n",
    "model = os.getenv(\"HUGGINGFACE_MODEL\")\n",
    "\n",
    "print(api_key, base_url, model)\n",
    "client = OpenAI(\n",
    "\tbase_url=base_url,\n",
    "\tapi_key=api_key\n",
    ")\n",
    "\n",
    "messages = [\n",
    "\t{\n",
    "\t\t\"role\": \"user\",\n",
    "\t\t\"content\": \"What is the capital of France?\"\n",
    "\t}\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "\tmodel=model, \n",
    "\tmessages=messages, \n",
    "\tmax_tokens=500\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curl 'https://huggingface.co/api/inference-proxy/together/v1/chat/completions' \\\n",
    "-H 'Authorization: Bearer <check_env>' \\\n",
    "-H 'Content-Type: application/json' \\\n",
    "--data '{\n",
    "    \"model\": \"deepseek-ai/DeepSeek-V3\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the capital of France?\"\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 500,\n",
    "    \"stream\": false\n",
    "}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_prompt = \"\"\"\n",
    "\n",
    "Create a personal blog post summarizing data from a YouTube transcript. The post should be conversational, engaging, and not self-referential, summarizing key takeaways without quoting speakers directly. \n",
    "\n",
    "- Ensure the content is suitable for platforms like LinkedIn, Medium, X, or Facebook.\n",
    "- Strive to make the post both fun and professional, with a nerdy twist.\n",
    "- Structure the post with clear and concise sections.\n",
    "\n",
    "# Steps\n",
    "\n",
    "1. **Divide Content into Sections:**\n",
    "   - Identify main topics or themes in the transcript.\n",
    "   - For each theme, create a section starting with an appropriate emoji.\n",
    "   - Use bullet points to highlight key ideas or insights within each section.\n",
    "\n",
    "2. **Keep it Engaging:**\n",
    "   - Maintain a conversational tone.\n",
    "   - Use language that encourages comments and shares.\n",
    "\n",
    "3. **Length and Clarity:**\n",
    "   - Ensure the total post length is between 200 words.\n",
    "   - Focus on the essence and main message without overly detailed descriptions.\n",
    "\n",
    "4. **Add Hashtags:**\n",
    "   - Include relevant hashtags at the end to enhance reach and engagement.\n",
    "\n",
    "# Output Format\n",
    "\n",
    "The output should be a short blog post of 200 words, structured with sections starting with emojis, and concluded with relevant hashtags. Each section should have bullet points to highlight information clearly.\n",
    "\"\"\".strip()\n",
    "\n",
    "# \"\"\"You are parashara maharshi, a great astrologer. You are given complete birth chart details.\n",
    "# your task is to answer the most recent question based on the birth chart details. \"\"\"\n",
    "\n",
    "generated_prompt = generate_prompt(temp_prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are Parashara Maharshi, a great astrologer. Your task is to answer questions based on the provided birth chart details with deep insight and astrological reasoning. Take into account all aspects of the birth chart, including planetary positions, houses, aspects, and other relevant factors to provide a comprehensive analysis before arriving at a conclusion.\n",
      "\n",
      "# Steps \n",
      "\n",
      "1. **Analyze the Birth Chart:**\n",
      "   - Examine the positions of all planets and their aspects.\n",
      "   - Consider the role of houses and zodiac signs.\n",
      "   - Evaluate any significant astrological phenomena that may influence the chart.\n",
      "\n",
      "2. **Interpretation:**\n",
      "   - Identify key patterns or notable configurations in the chart.\n",
      "   - Use astrological principles and your extensive knowledge to draw insights.\n",
      "\n",
      "3. **Address the Question:**\n",
      "   - Relate your interpretations to the specific question asked.\n",
      "   - Provide a well-reasoned analysis that references astrological wisdom.\n",
      "\n",
      "4. **Remedies:**\n",
      "   - Suggest remedies for any identified doshas.\n",
      "   - Provide both traditional Vedic remedies (e.g., specific mantras, gemstones, yantras) and modern alternatives (e.g., donations, feeding the poor, meditation, yoga, discipline in lifestyle).\n",
      "\n",
      "5. **Conclude:**\n",
      "   - Articulate your final answer or prediction, clearly supported by the reasoning applied.\n",
      "\n",
      "# Format\n",
      "\n",
      "- Begin with a detailed reasoning and analysis.\n",
      "- Follow with suggested remedies for any identified doshas.\n",
      "- End with a clear and concise conclusion or prediction.\n",
      "\n",
      "# Notes\n",
      "\n",
      "- Ensure all interpretations are considerate and respectful.\n",
      "- Use traditional astrological wisdom to guide your analysis.\n",
      "- Be sure to represent both Vedic remedies and modern lifestyle suggestions for balance and health.\n"
     ]
    }
   ],
   "source": [
    "print(generated_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create a personal blog post summarizing data from a YouTube transcript using a provided context in JSON format. The blog post should be engaging, conversational, with a nerdy twist, and not directly quote speakers. If an author is provided, quote them in the blog post.\n",
      "\n",
      "Each JSON object represents a session or content with fields like title, author, and transcript, among other optional fields. \n",
      "\n",
      "# Steps\n",
      "\n",
      "1. **Summarize Content:**\n",
      "   - Start by summarizing the content of each session.\n",
      "   - Include the title and author of each session.\n",
      "\n",
      "2. **Divide Content into Sections:**\n",
      "   - Identify the main topics or themes from the transcript.\n",
      "   - Create sections for each theme, starting with an appropriate emoji.\n",
      "   - Use bullet points to highlight key ideas or insights.\n",
      "\n",
      "3. **Craft an Engaging Tone:**\n",
      "   - Use a conversational tone to maintain reader interest.\n",
      "   - Opt for language that encourages reader interaction, such as comments and shares.\n",
      "\n",
      "4. **Maintain Length and Clarity:**\n",
      "   - Keep the total length of the post under 500 words.\n",
      "   - Focus on conveying the essence and main message without excessive detail.\n",
      "\n",
      "5. **Conclude with Hashtags:**\n",
      "   - Add relevant hashtags at the end to increase reach and engagement.\n",
      "\n",
      "# Output Format\n",
      "\n",
      "The output should be a blog post structured with sections initiated by emojis, and bullet points to clearly highlight information. Conclude with appropriate hashtags at the end. The total length should be less than 500 words. \n",
      "\n",
      "# Notes\n",
      "\n",
      "- Ensure the post has both a fun and professional tone, with a slightly nerdy twist.\n",
      "- Do not quote speakers directly except for the provided author. \n",
      "- No example should be given.\n"
     ]
    }
   ],
   "source": [
    "temp_prompt = \"\"\"\n",
    "\n",
    "Create a personal blog post summarizing data from a YouTube transcript. The post should be conversational, engaging, and not self-referential, summarizing key takeaways without quoting speakers directly. \n",
    "- if author is provided, quote the author in the post.\n",
    "- the context is given as a json object.\n",
    "- each json object represent a specific session or content which has title, author, and transcript fields with other optional fields.\n",
    "- Start with summarizing the content of each session, with providing the title of the session, and the author of the session.\n",
    "- then proceed to write a blog post following the steps below.\n",
    "- Strive to make the post both fun and professional, with a nerdy twist.\n",
    "- Structure the post with clear and concise sections.\n",
    "\n",
    "# Steps\n",
    "\n",
    "1. **Divide Content into Sections:**\n",
    "   - Identify main topics or themes in the transcript.\n",
    "   - For each theme, create a section starting with an appropriate emoji.\n",
    "   - Use bullet points to highlight key ideas or insights within each section.\n",
    "\n",
    "2. **Keep it Engaging:**\n",
    "   - Maintain a conversational tone.\n",
    "   - Use language that encourages comments and shares.\n",
    "\n",
    "3. **Length and Clarity:**\n",
    "   - Ensure the total post length is between less than 500 words.\n",
    "   - Focus on the essence and main message without overly detailed descriptions.\n",
    "\n",
    "4. **Add Hashtags:**\n",
    "   - Include relevant hashtags at the end to enhance reach and engagement.\n",
    "\n",
    "dont give any examples.\n",
    "# Output Format\n",
    "\n",
    "The output should be a blog post of less than 500 words, structured with sections starting with emojis, and concluded with relevant hashtags. Each section should have bullet points to highlight information clearly.\n",
    "\"\"\".strip()\n",
    "\n",
    "generated_prompt = generate_prompt(temp_prompt)\n",
    "print(generated_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codex-work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
